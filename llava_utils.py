from transformers import BlipProcessor, BlipForConditionalGeneration
from PIL import Image
import torch
from deep_translator import GoogleTranslator
from model_utils import generate_response, load_model
from transformers import LlavaProcessor
from transformers import LlavaNextProcessor, LlavaOnevisionForConditionalGeneration,AutoProcessor

def load_llava_model():
    model_id = "llava-hf/llava-onevision-qwen2-0.5b-ov-hf"
    
    model = LlavaOnevisionForConditionalGeneration.from_pretrained(
        model_id,
        torch_dtype=torch.float16,
        low_cpu_mem_usage=True,

    ).to(0 if torch.cuda.is_available() else "cpu")

    processor = AutoProcessor.from_pretrained(model_id)
    return processor, model

def analyze_image(processor, model, image: Image.Image, question: str):
    # KonuÅŸma ÅŸablonu
    conversation = [
        {
            "role": "user",
            "content": [
                {"type": "text", "text": question},
                {"type": "image"},
            ],
        }
    ]
    
    prompt = processor.apply_chat_template(conversation, add_generation_prompt=True)
    
    inputs = processor(images=image, text=prompt, return_tensors="pt").to(model.device, torch.float16)

    with torch.no_grad():
        outputs = model.generate(**inputs, max_new_tokens=150, do_sample=False)

    return processor.decode(outputs[0][2:], skip_special_tokens=True)
# ðŸ”¹ BLIP-2 Modelini YÃ¼kleme (GPU destekli)
def load_blip2_model():
    model_id = "Salesforce/blip-image-captioning-base"
    processor = BlipProcessor.from_pretrained(model_id)
    model = BlipForConditionalGeneration.from_pretrained(model_id)
    model.to("cuda" if torch.cuda.is_available() else "cpu")
    return processor, model

# ðŸ”¹ BLIP-2 GÃ¶rsel Analizi
def analyze_with_blip2(processor, model, image: Image.Image, question: str = "What is in the image?"):
    device = "cuda" if torch.cuda.is_available() else "cpu"
    inputs = processor(images=image, text=question, return_tensors="pt").to(device)
    with torch.no_grad():
        out = model.generate(**inputs, max_new_tokens=100)
    return processor.tokenizer.decode(out[0], skip_special_tokens=True)

def translate_and_summarize(english_caption: str, selected_model_name: str):
    # 1. Ä°ngilizce'yi TÃ¼rkÃ§eye Ã§evir
    try:
        turkish_caption = GoogleTranslator(source='en', target='tr').translate(english_caption)
    except Exception as e:
        return f"Ã‡eviri sÄ±rasÄ±nda hata oluÅŸtu: {str(e)}"
    
    # 2. Pazarlama odaklÄ± iÃ§erik Ã¼ret
    prompt = f"""
AÅŸaÄŸÄ±daki aÃ§Ä±klamayÄ± bir pazarlama metnine dÃ¶nÃ¼ÅŸtÃ¼r.
1. ÃœrÃ¼nÃ¼n duygusal veya iÅŸlevsel faydasÄ±nÄ± vurgula.
2. Hedef kitleyi dÃ¼ÅŸÃ¼nerek yaz.
3. Etkileyici bir Ã§aÄŸrÄ± (CTA - Call To Action) ekle.
4. SonuÃ§ TÃ¼rkÃ§e ve sade bir dille olsun.

ÃœrÃ¼n aÃ§Ä±klamasÄ±: {turkish_caption}
"""
    try:
        summary = generate_response(selected_model_name, prompt)
        return summary
    except Exception as e:
        return f"Pazarlama metni Ã¼retilemedi: {str(e)}"
